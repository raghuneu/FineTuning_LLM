{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19795b26",
   "metadata": {},
   "source": [
    "# Fine-Tuning RoBERTa for Legal Contract Clause Extraction\n",
    "## Project Overview\n",
    "This project fine-tunes a RoBERTa model to automatically extract 6 key clause types from legal contracts using the CUAD dataset.\n",
    "\n",
    "**Target Clauses:**\n",
    "- Governing Law\n",
    "- Expiration Date\n",
    "- Effective Date\n",
    "- Anti-Assignment\n",
    "- Cap On Liability\n",
    "- License Grant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bfc97",
   "metadata": {},
   "source": [
    "## Phase 1: Data Preparation\n",
    "Loading and formatting the CUAD dataset for question-answering task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b834ef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in dataset: dict_keys(['version', 'data'])\n",
      "Number of entries: 510\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the CUAD dataset\n",
    "with open('../data/CUADv1.json', 'r') as f:\n",
    "    cuad_data = json.load(f)\n",
    "\n",
    "# Check the structure\n",
    "print(f\"Keys in dataset: {cuad_data.keys()}\")\n",
    "print(f\"Number of entries: {len(cuad_data['data'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fae06b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: LIMEENERGYCO_09_09_1999-EX-10-DISTRIBUTOR AGREEMENT\n",
      "\n",
      "Number of paragraphs: 1\n",
      "\n",
      "First paragraph keys: dict_keys(['qas', 'context'])\n"
     ]
    }
   ],
   "source": [
    "# Explore first entry\n",
    "first_entry = cuad_data['data'][0]\n",
    "print(f\"Title: {first_entry['title']}\")\n",
    "print(f\"\\nNumber of paragraphs: {len(first_entry['paragraphs'])}\")\n",
    "print(f\"\\nFirst paragraph keys: {first_entry['paragraphs'][0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85812d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contract text length: 54290 characters\n",
      "\n",
      "Number of questions (clause types): 41\n",
      "\n",
      "First question example:\n",
      "Question: Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract\n",
      "Answer: [{'text': 'DISTRIBUTOR AGREEMENT', 'answer_start': 44}]\n"
     ]
    }
   ],
   "source": [
    "# Look at the context and questions\n",
    "first_para = first_entry['paragraphs'][0]\n",
    "\n",
    "print(f\"Contract text length: {len(first_para['context'])} characters\")\n",
    "print(f\"\\nNumber of questions (clause types): {len(first_para['qas'])}\")\n",
    "print(f\"\\nFirst question example:\")\n",
    "print(f\"Question: {first_para['qas'][0]['question']}\")\n",
    "print(f\"Answer: {first_para['qas'][0]['answers']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "801046aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most common clause types:\n",
      "\n",
      "510 contracts: Highlight the parts (if any) of this contract related to \"Document Name\" that should be reviewed by a lawyer. Details: The name of the contract\n",
      "509 contracts: Highlight the parts (if any) of this contract related to \"Parties\" that should be reviewed by a lawyer. Details: The two or more parties who signed the contract\n",
      "470 contracts: Highlight the parts (if any) of this contract related to \"Agreement Date\" that should be reviewed by a lawyer. Details: The date of the contract\n",
      "437 contracts: Highlight the parts (if any) of this contract related to \"Governing Law\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?\n",
      "413 contracts: Highlight the parts (if any) of this contract related to \"Expiration Date\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?\n",
      "390 contracts: Highlight the parts (if any) of this contract related to \"Effective Date\" that should be reviewed by a lawyer. Details: The date when the contract is effective \n",
      "374 contracts: Highlight the parts (if any) of this contract related to \"Anti-Assignment\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?\n",
      "275 contracts: Highlight the parts (if any) of this contract related to \"Cap On Liability\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.\n",
      "255 contracts: Highlight the parts (if any) of this contract related to \"License Grant\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?\n",
      "214 contracts: Highlight the parts (if any) of this contract related to \"Audit Rights\" that should be reviewed by a lawyer. Details: Does a party have the right to  audit the books, records, or physical locations of the counterparty to ensure compliance with the contract?\n"
     ]
    }
   ],
   "source": [
    "# Count how many contracts have answers for each clause type\n",
    "clause_counts = {}\n",
    "\n",
    "for entry in cuad_data['data']:\n",
    "    for para in entry['paragraphs']:\n",
    "        for qa in para['qas']:\n",
    "            question = qa['question']\n",
    "            has_answer = len(qa['answers']) > 0\n",
    "            \n",
    "            if question not in clause_counts:\n",
    "                clause_counts[question] = 0\n",
    "            if has_answer:\n",
    "                clause_counts[question] += 1\n",
    "\n",
    "# Show top 10 most common clauses\n",
    "sorted_clauses = sorted(clause_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 most common clause types:\\n\")\n",
    "for clause, count in sorted_clauses[:10]:\n",
    "    print(f\"{count} contracts: {clause}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "260ee6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 6 clause types for extraction\n"
     ]
    }
   ],
   "source": [
    "# Define target clause types\n",
    "target_clauses = [\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"Governing Law\\\" that should be reviewed by a lawyer. Details: Which state/country's law governs the interpretation of the contract?\",\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"Expiration Date\\\" that should be reviewed by a lawyer. Details: On what date will the contract's initial term expire?\",\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"Effective Date\\\" that should be reviewed by a lawyer. Details: The date when the contract is effective \",\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"Anti-Assignment\\\" that should be reviewed by a lawyer. Details: Is consent or notice required of a party if the contract is assigned to a third party?\",\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"Cap On Liability\\\" that should be reviewed by a lawyer. Details: Does the contract include a cap on liability upon the breach of a party’s obligation? This includes time limitation for the counterparty to bring claims or maximum amount for recovery.\",\n",
    "    \"Highlight the parts (if any) of this contract related to \\\"License Grant\\\" that should be reviewed by a lawyer. Details: Does the contract contain a license granted by one party to its counterparty?\"\n",
    "]\n",
    "\n",
    "print(f\"Selected {len(target_clauses)} clause types for extraction\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "77ca7793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 3060\n"
     ]
    }
   ],
   "source": [
    "# Extract data using partial matching on clause names\n",
    "clause_names = [\"Governing Law\", \"Expiration Date\", \"Effective Date\", \n",
    "                \"Anti-Assignment\", \"Cap On Liability\", \"License Grant\"]\n",
    "\n",
    "filtered_data = []\n",
    "\n",
    "for entry in cuad_data['data']:\n",
    "    contract_id = entry['title']\n",
    "    for para in entry['paragraphs']:\n",
    "        context = para['context']\n",
    "        \n",
    "        for qa in para['qas']:\n",
    "            # Check if any of our clause names appears in the question\n",
    "            for clause_name in clause_names:\n",
    "                if f'\"{clause_name}\"' in qa['question']:\n",
    "                    filtered_data.append({\n",
    "                        'contract_id': contract_id,\n",
    "                        'context': context,\n",
    "                        'question': qa['question'],\n",
    "                        'answers': qa['answers']\n",
    "                    })\n",
    "                    break  # Found this clause, move to next qa\n",
    "\n",
    "print(f\"Total examples: {len(filtered_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ab24bfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples per clause type:\n",
      "\n",
      "Effective Date: 510 examples\n",
      "Expiration Date: 510 examples\n",
      "Governing Law: 510 examples\n",
      "Anti-Assignment: 510 examples\n",
      "License Grant: 510 examples\n",
      "Cap On Liability: 510 examples\n"
     ]
    }
   ],
   "source": [
    "# Count examples per clause type\n",
    "from collections import Counter\n",
    "\n",
    "clause_distribution = Counter([item['question'] for item in filtered_data])\n",
    "\n",
    "print(\"Examples per clause type:\\n\")\n",
    "for question, count in clause_distribution.items():\n",
    "    # Extract just the clause name for readability\n",
    "    clause_name = question.split('\"')[1]\n",
    "    print(f\"{clause_name}: {count} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9c632d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples WITH answers: 2144\n",
      "Examples WITHOUT answers: 916\n",
      "Percentage with answers: 70.1%\n"
     ]
    }
   ],
   "source": [
    "# Count examples with vs without answers\n",
    "has_answer = sum(1 for item in filtered_data if len(item['answers']) > 0)\n",
    "no_answer = len(filtered_data) - has_answer\n",
    "\n",
    "print(f\"Examples WITH answers: {has_answer}\")\n",
    "print(f\"Examples WITHOUT answers: {no_answer}\")\n",
    "print(f\"Percentage with answers: {has_answer/len(filtered_data)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8e9e4b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clause: Effective Date\n",
      "\n",
      "Extracted text: The term of this  Agreement  shall be ten (10)                            years (the \"Term\")  which shall  commence on the date                            upon which the Company  delivers to  Distributor  the                            last Sample, as defined  hereinafter.\n",
      "Position in contract: character 5268\n"
     ]
    }
   ],
   "source": [
    "# Show one example with an answer\n",
    "example_with_answer = [item for item in filtered_data if len(item['answers']) > 0][0]\n",
    "\n",
    "clause_name = example_with_answer['question'].split('\"')[1]\n",
    "answer_text = example_with_answer['answers'][0]['text']\n",
    "answer_start = example_with_answer['answers'][0]['answer_start']\n",
    "\n",
    "print(f\"Clause: {clause_name}\")\n",
    "print(f\"\\nExtracted text: {answer_text}\")\n",
    "print(f\"Position in contract: character {answer_start}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23ac0593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total formatted examples: 3060\n",
      "\n",
      "First example keys: dict_keys(['id', 'context', 'question', 'answers'])\n"
     ]
    }
   ],
   "source": [
    "# Format data for QA task\n",
    "formatted_examples = []\n",
    "\n",
    "for item in filtered_data:\n",
    "    example = {\n",
    "        'id': f\"{item['contract_id']}_{item['question'].split('\\\"')[1]}\",\n",
    "        'context': item['context'],\n",
    "        'question': item['question'],\n",
    "        'answers': item['answers']\n",
    "    }\n",
    "    formatted_examples.append(example)\n",
    "\n",
    "print(f\"Total formatted examples: {len(formatted_examples)}\")\n",
    "print(f\"\\nFirst example keys: {formatted_examples[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b7f5e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'context', 'question', 'answers'])\n"
     ]
    }
   ],
   "source": [
    "# Check the actual keys in our formatted examples\n",
    "print(formatted_examples[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d63f90",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split\n",
    "Splitting by contract ID to prevent data leakage (70/15/15 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d1017ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique contracts: 510\n",
      "Train contracts: 357\n",
      "Val contracts: 76\n",
      "Test contracts: 77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract contract IDs from the 'id' field\n",
    "unique_contracts = list(set([item['id'].rsplit('_', 1)[0] for item in formatted_examples]))\n",
    "print(f\"Total unique contracts: {len(unique_contracts)}\")\n",
    "\n",
    "# Split contract IDs\n",
    "train_ids, temp_ids = train_test_split(unique_contracts, test_size=0.3, random_state=42)\n",
    "val_ids, test_ids = train_test_split(temp_ids, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Train contracts: {len(train_ids)}\")\n",
    "print(f\"Val contracts: {len(val_ids)}\")\n",
    "print(f\"Test contracts: {len(test_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "727763c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples: 2142 (357 contracts × 6 clauses)\n",
      "Val examples: 456 (76 contracts × 6 clauses)\n",
      "Test examples: 462 (77 contracts × 6 clauses)\n"
     ]
    }
   ],
   "source": [
    "# Split examples based on contract IDs\n",
    "train_data = [ex for ex in formatted_examples if ex['id'].rsplit('_', 1)[0] in train_ids]\n",
    "val_data = [ex for ex in formatted_examples if ex['id'].rsplit('_', 1)[0] in val_ids]\n",
    "test_data = [ex for ex in formatted_examples if ex['id'].rsplit('_', 1)[0] in test_ids]\n",
    "\n",
    "print(f\"Train examples: {len(train_data)} ({len(train_data)/6:.0f} contracts × 6 clauses)\")\n",
    "print(f\"Val examples: {len(val_data)} ({len(val_data)/6:.0f} contracts × 6 clauses)\")\n",
    "print(f\"Test examples: {len(test_data)} ({len(test_data)/6:.0f} contracts × 6 clauses)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "152d9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved train_data.json\n",
      "✓ Saved val_data.json\n",
      "✓ Saved test_data.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save datasets\n",
    "with open('../data/train_data.json', 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "\n",
    "with open('../data/val_data.json', 'w') as f:\n",
    "    json.dump(val_data, f, indent=2)\n",
    "\n",
    "with open('../data/test_data.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "print(\"✓ Saved train_data.json\")\n",
    "print(\"✓ Saved val_data.json\")\n",
    "print(\"✓ Saved test_data.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e8322c",
   "metadata": {},
   "source": [
    "## Phase 2: Model Selection & Setup\n",
    "Using RoBERTa-base for question-answering on legal contracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "398ab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ transformers version: 5.1.0\n"
     ]
    }
   ],
   "source": [
    "# Check if transformers is installed\n",
    "try:\n",
    "    import transformers\n",
    "    print(f\"✓ transformers version: {transformers.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"Need to install: !pip install transformers datasets accelerate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd7034",
   "metadata": {},
   "source": [
    "### Loading RoBERTa Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1a9fb7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dec25649e8f45d99bc9b76f71bf7d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1mRobertaForQuestionAnswering LOAD REPORT\u001b[0m from: roberta-base\n",
      "Key                             | Status     | \n",
      "--------------------------------+------------+-\n",
      "lm_head.layer_norm.bias         | UNEXPECTED | \n",
      "lm_head.dense.weight            | UNEXPECTED | \n",
      "roberta.embeddings.position_ids | UNEXPECTED | \n",
      "lm_head.layer_norm.weight       | UNEXPECTED | \n",
      "lm_head.dense.bias              | UNEXPECTED | \n",
      "lm_head.bias                    | UNEXPECTED | \n",
      "qa_outputs.bias                 | MISSING    | \n",
      "qa_outputs.weight               | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded roberta-base\n",
      "Model parameters: 124,056,578\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "\n",
    "model_name = \"roberta-base\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "\n",
    "print(f\"✓ Loaded {model_name}\")\n",
    "print(f\"Model parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae93d5",
   "metadata": {},
   "source": [
    "### Tokenizing the Dataset\n",
    "Converting text to token IDs and mapping answer positions to token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dc5918e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created HF datasets\n",
      "Train: 2142 examples\n",
      "Val: 456 examples\n",
      "Test: 462 examples\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Convert our lists to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(f\"✓ Created HF datasets\")\n",
    "print(f\"Train: {len(train_dataset)} examples\")\n",
    "print(f\"Val: {len(val_dataset)} examples\")\n",
    "print(f\"Test: {len(test_dataset)} examples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c48f33",
   "metadata": {},
   "source": [
    "### Preprocessing Function\n",
    "Maps character positions to token positions for answer spans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44bd0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenizes questions and contexts, and maps answer character positions to token positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    \n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    \n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        \n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        \n",
    "        # Check if answers is a list (batched) or dict (single)\n",
    "        if isinstance(answers, list):\n",
    "            has_answer = len(answers) > 0 and answers[0].get(\"text\")\n",
    "        else:\n",
    "            has_answer = len(answers.get(\"text\", [])) > 0\n",
    "        \n",
    "        if not has_answer:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            if isinstance(answers, list):\n",
    "                start_char = answers[0][\"answer_start\"]\n",
    "                answer_text = answers[0][\"text\"]\n",
    "            else:\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                answer_text = answers[\"text\"][0]\n",
    "            \n",
    "            end_char = start_char + len(answer_text)\n",
    "            \n",
    "            token_start_index = 0\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "            \n",
    "            token_end_index = len(offsets) - 1\n",
    "            while token_end_index >= 0 and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "    \n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    \n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b49579",
   "metadata": {},
   "source": [
    "### Apply Tokenization to All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "75d16c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e18b6f1ee91745a5acc54f98141ea341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2142 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0355db57c3a641a38185ccab81ffe8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/456 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Tokenized train: 126982 examples\n",
      "✓ Tokenized val: 31702 examples\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all datasets\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"✓ Tokenized train: {len(tokenized_train)} examples\")\n",
    "print(f\"✓ Tokenized val: {len(tokenized_val)} examples\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
